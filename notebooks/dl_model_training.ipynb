{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f83a7586",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yaml\n",
    "\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.metrics import classification_report\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f92f512b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Прочтём файл конфига с путями\n",
    "\n",
    "CONFIG_PATH = \"config.yaml\"\n",
    "with open(CONFIG_PATH, \"r\", encoding=\"utf-8\") as config_file:\n",
    "    CONFIG = yaml.load(config_file, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc68c6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузим датасет постов и обработаем тексты\n",
    "\n",
    "post_df = pd.read_csv(CONFIG['data_folder'] + '/post_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074267e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "post_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4452baca",
   "metadata": {},
   "source": [
    "### Предобработаем и лемматизируем text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f5ebd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = re.findall('[^\\W_]+', text)\n",
    "    text = [token.lower() for token in text if len(token) > 1]\n",
    "    text = \" \".join(text)\n",
    "    if len(text) == 0:\n",
    "        return 'placeholder text'\n",
    "    return text\n",
    "\n",
    "def lemmatize_row(row: str, lemmatizer: WordNetLemmatizer) -> str:\n",
    "    list_of_words = row.split()\n",
    "    list_of_words = list(map(lemmatizer.lemmatize, list_of_words))\n",
    "    return ' '.join(list_of_words)\n",
    "\n",
    "def lemmatize_text_column(df: pd.DataFrame, column: str) -> pd.DataFrame:\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_col = f'lemmatized_{column}'\n",
    "    df[lemmatized_col] = df[column].apply(lambda x: preprocess_text(x))\n",
    "    df[lemmatized_col] = df[lemmatized_col].apply(\n",
    "        lambda x: lemmatize_row(x, lemmatizer)\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d44c47df",
   "metadata": {},
   "outputs": [],
   "source": [
    "post_df = lemmatize_text_column(post_df, 'text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3f03b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "post_df[['lemmatized_text', 'text']].sample(1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d277d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "post_df.to_csv(CONFIG['datasets_folder'] + '/post_data_lemmatized_and_embs.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a6c56c",
   "metadata": {},
   "source": [
    "### Text w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "236d3c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossLogger(CallbackAny2Vec):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.epoch = 0\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        loss = model.get_latest_training_loss()\n",
    "        if self.epoch == 0:\n",
    "            print('Loss after epoch {}: {}'.format(self.epoch, loss))\n",
    "        elif self.epoch % 10 == 0:\n",
    "            print('Loss after epoch {}: {}'.format(self.epoch, loss - self.loss_previous_step))\n",
    "        self.epoch += 1\n",
    "        self.loss_previous_step = loss\n",
    "        \n",
    "\n",
    "class EpochLogger(CallbackAny2Vec):\n",
    "    def __init__(self):\n",
    "        self.epoch = 0\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        print(f'Epoch {self.epoch}')\n",
    "        self.epoch += 1\n",
    "        \n",
    "class Word2VecTransformer(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, w2v_model, alpha=1):\n",
    "        \n",
    "        self.w2v_model = w2v_model\n",
    "        self.alpha = alpha\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        X_transformed = np.zeros((len(X), self.w2v_model.wv.vector_size))\n",
    "        for i, title in enumerate(X):\n",
    "            title_vector = np.zeros((self.w2v_model.wv.vector_size,))\n",
    "            try:\n",
    "                tokens = title.split()\n",
    "            except BaseException:\n",
    "                continue\n",
    "            \n",
    "            counter = 1\n",
    "            \n",
    "            for token in tokens:\n",
    "                if token in self.w2v_model.wv.key_to_index:\n",
    "                    title_vector += self.w2v_model.wv.get_vector(token)\n",
    "                    counter += 1 \n",
    "                    \n",
    "            X_transformed[i] = title_vector / (self.alpha * counter)\n",
    "        \n",
    "        return X_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753b936e",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_w2v_corpus = post_df.lemmatized_text.str.split()\n",
    "text_w2v_corpus = text_w2v_corpus\n",
    "text_w2v_model = Word2Vec(sg=1, hs=1, vector_size=10)\n",
    "text_w2v_model.build_vocab(text_w2v_corpus)\n",
    "text_w2v_model.train(\n",
    "    text_w2v_corpus,\n",
    "    total_examples=text_w2v_model.corpus_count,\n",
    "    epochs=250,\n",
    "    compute_loss=True,\n",
    "    callbacks=[LossLogger()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a7ce139",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_w2v_model.save(CONFIG['datasets_folder'] + '/text_w2v_model_10')\n",
    "text_word_vectors = text_w2v_model.wv\n",
    "text_word_vectors.save(CONFIG['datasets_folder'] + '/text_w2v_word_vectors_10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db9d775",
   "metadata": {},
   "outputs": [],
   "source": [
    "text2vec = Word2VecTransformer(w2v_model=text_w2v_model)\n",
    "w2v_text_transform = text2vec.transform(post_df.lemmatized_text.values)\n",
    "\n",
    "text_w2v_cols = [f'text_w2v_{i}' for i in range(1, 11)]\n",
    "\n",
    "w2v_df = pd.DataFrame(w2v_text_transform, columns=text_w2v_cols)\n",
    "post_df = pd.concat((post_df.reset_index(drop=True), w2v_df.reset_index(drop=True)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2826f5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "post_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d16173bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "post_df.to_csv(CONFIG['datasets_folder'] + '/post_data_lemmatized_and_embs.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681adaaf",
   "metadata": {},
   "source": [
    "## Обучим катбуст"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "82e9cc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Соединим исходный датасет действий пользователей с полученными эмбеддингами текстов\n",
    "\n",
    "data = pd.read_csv(CONFIG['datasets_folder'] + '/processed_df.csv')\n",
    "post_df = pd.read_csv(CONFIG['datasets_folder'] + '/post_data_lemmatized_and_embs.csv').drop(['topic'], axis=1)\n",
    "data = data.merge(post_df, on='post_id')\n",
    "\n",
    "text_w2v_cols = [f'text_w2v_{i}' for i in range(1, 11)]\n",
    "\n",
    "columns_order = [\n",
    "    'post_id', 'topic', 'tfidf_sum', \n",
    "    'tfidf_mean', 'tfidf_max', 'user_id',\n",
    "    'gender', 'age', 'country', 'city', \n",
    "    'exp_group', 'os', 'source','month', \n",
    "    'hour', 'day', 'weekday', 'timestamp', 'target'\n",
    "] + text_w2v_cols\n",
    "data = data[columns_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77ad36a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Разделим датасет на трейн и тест\n",
    "\n",
    "X = data.drop(['timestamp', 'target', 'user_id', 'post_id'], axis=1)\n",
    "y = data['target']\n",
    "\n",
    "X_train = X.iloc[:-712175].copy()\n",
    "y_train = y.iloc[:-712175].copy()\n",
    "\n",
    "X_test = X.iloc[-712175:].copy()\n",
    "y_test = y.iloc[-712175:].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b6a7618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ввиду дисбаланса классов, найдем их веса\n",
    "\n",
    "classes = np.unique(y_train)\n",
    "weights = compute_class_weight(class_weight='balanced', classes=classes, y=y_train)\n",
    "class_weights = dict(zip(classes, weights))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cb9728cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.5721843779474648, 1: 3.963353250504535}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0aadeb1d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c7088a3727347509e72b4d5245f738d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MetricVisualizer(layout=Layout(align_self='stretch', height='500px'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostClassifier at 0x1812d4cd990>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Обучим катбуст\n",
    "\n",
    "categorical_features = ['country', 'city', 'topic']\n",
    "catboost_model = CatBoostClassifier(\n",
    "    class_weights=class_weights, \n",
    "    cat_features=categorical_features, \n",
    "    verbose=False\n",
    ")\n",
    "\n",
    "catboost_model.fit(X_train, y_train, plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9926401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Качество на тесте: 0.6165787903254116\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'0': {'precision': 0.9221623049854076,\n",
       "  'recall': 0.6146808674155576,\n",
       "  'f1-score': 0.7376621579947025,\n",
       "  'support': 624568},\n",
       " '1': {'precision': 0.18658149124585952,\n",
       "  'recall': 0.6301094661385506,\n",
       "  'f1-score': 0.2879100418028148,\n",
       "  'support': 87607},\n",
       " 'accuracy': 0.6165787903254116,\n",
       " 'macro avg': {'precision': 0.5543718981156336,\n",
       "  'recall': 0.6223951667770541,\n",
       "  'f1-score': 0.5127860998987587,\n",
       "  'support': 712175},\n",
       " 'weighted avg': {'precision': 0.831676078497142,\n",
       "  'recall': 0.6165787903254116,\n",
       "  'f1-score': 0.6823366640596126,\n",
       "  'support': 712175}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Грубо оценим качество обученной модели\n",
    "\n",
    "print(f\"Качество на тесте: {catboost_model.score(X_test, y_test)}\")\n",
    "classification_report(y_test, catboost_model.predict(X_test), output_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fe2e5bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "catboost_model.save_model(CONFIG['data_folder'] + '/catboost_model_with_text_embs', format=\"cbm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba797ad3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
