{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f83a7586",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import yaml\n",
    "\n",
    "from catboost import CatBoostClassifier\n",
    "from sklearn.metrics import classification_report, roc_auc_score\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.models.callbacks import CallbackAny2Vec\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "from joblib import Parallel, delayed\n",
    "from tqdm import tqdm\n",
    "from sklearn.base import BaseEstimator, TransformerMixin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f92f512b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Прочтём файл конфига с путями\n",
    "\n",
    "CONFIG_PATH = \"config.yaml\"\n",
    "with open(CONFIG_PATH, \"r\", encoding=\"utf-8\") as config_file:\n",
    "    CONFIG = yaml.load(config_file, Loader=yaml.FullLoader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dc68c6fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Загрузим датасет постов и обработаем тексты\n",
    "\n",
    "post_df = pd.read_csv(CONFIG['data_folder'] + '/post_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "074267e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>text</th>\n",
       "      <th>topic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>UK economy facing major risks  The UK manufact...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Aids and climate top Davos agenda  Climate cha...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Asian quake hits European shares  Shares in Eu...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>India power shares jump on debut  Shares in In...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Lacroix label bought by US firm  Luxury goods ...</td>\n",
       "      <td>business</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   post_id                                               text     topic\n",
       "0        1  UK economy facing major risks  The UK manufact...  business\n",
       "1        2  Aids and climate top Davos agenda  Climate cha...  business\n",
       "2        3  Asian quake hits European shares  Shares in Eu...  business\n",
       "3        4  India power shares jump on debut  Shares in In...  business\n",
       "4        5  Lacroix label bought by US firm  Luxury goods ...  business"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4452baca",
   "metadata": {},
   "source": [
    "### Предобработаем и лемматизируем text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "40f5ebd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text):\n",
    "    text = re.findall('[^\\W_]+', text)\n",
    "    text = [token.lower() for token in text if len(token) > 1]\n",
    "    text = \" \".join(text)\n",
    "    if len(text) == 0:\n",
    "        return 'placeholder text'\n",
    "    return text\n",
    "\n",
    "def lemmatize_row(row: str, lemmatizer: WordNetLemmatizer) -> str:\n",
    "    list_of_words = row.split()\n",
    "    list_of_words = list(map(lemmatizer.lemmatize, list_of_words))\n",
    "    return ' '.join(list_of_words)\n",
    "\n",
    "def lemmatize_text_column(df: pd.DataFrame, column: str) -> pd.DataFrame:\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_col = f'lemmatized_{column}'\n",
    "    df[lemmatized_col] = df[column].apply(lambda x: preprocess_text(x))\n",
    "    df[lemmatized_col] = df[lemmatized_col].apply(\n",
    "        lambda x: lemmatize_row(x, lemmatizer)\n",
    "    )\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d44c47df",
   "metadata": {},
   "outputs": [],
   "source": [
    "post_df = lemmatize_text_column(post_df, 'text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5a3f03b4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['uk young top euro earnings league british child enjoy the highest average annual income in europe more than double that of spanish or italian youngster report suggests child in the uk between the age of 10 and 17 had an annual income of 775 said market analyst datamonitor they use pester power to get their parent to stump up nearly third of this income the report said a for how they spend their cash the bulk go on personal care soft drink and food datamonitor said datamonitor add that british teenager are keen on personal care because it help them combine two seemingly contradictory emotional need the desire to fit in and the desire to express their individuality british teenage girl compared to their counterpart in seven european country are the most keen to use make up product nearly three out of four girl said they used make up according to the datamonitor report the trend marked british teenager out a particularly important to cosmetic manufacturer a they are likely to experiment more with brand and product and form long term beauty routine and the good time are likely to keep rolling for british child a the report predicts that they will still be topping the earnings table in 2008',\n",
       "        'UK young top Euro earnings league  British children enjoy the highest average annual income in Europe - more than double that of Spanish or Italian youngsters, a report suggests.  Children in the UK between the ages of 10 and 17 had an annual income of £775, said market analyst Datamonitor. They use pester power to get their parents to stump up nearly a third of this income, the report said. As for how they spend their cash, the bulk goes on personal care, soft drinks and food, Datamonitor said.  Datamonitor adds that British teenagers are keen on personal care because it helps them combine two seemingly contradictory emotional needs - the desire to fit in and the desire to express their individuality.  British teenage girls, compared to their counterparts in seven European countries, are the most keen to use make-up products. Nearly three out of four girls said they used make-up. According to the Datamonitor report the trend marked British teenagers out as particularly important to cosmetics manufacturers as they are likely to experiment more with brands and products and form long-term beauty routines. And the good times are likely to keep rolling for British children, as the report predicts that they will still be topping the earnings table in 2008. ']],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_df[['lemmatized_text', 'text']].sample(1).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "00d277d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "post_df.to_csv(CONFIG['datasets_folder'] + '/post_data_lemmatized_and_embs.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a6c56c",
   "metadata": {},
   "source": [
    "### Text w2v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "236d3c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossLogger(CallbackAny2Vec):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.epoch = 0\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        loss = model.get_latest_training_loss()\n",
    "        if self.epoch == 0:\n",
    "            print('Loss after epoch {}: {}'.format(self.epoch, loss))\n",
    "        elif self.epoch % 10 == 0:\n",
    "            print('Loss after epoch {}: {}'.format(self.epoch, loss - self.loss_previous_step))\n",
    "        self.epoch += 1\n",
    "        self.loss_previous_step = loss\n",
    "        \n",
    "\n",
    "class EpochLogger(CallbackAny2Vec):\n",
    "    def __init__(self):\n",
    "        self.epoch = 0\n",
    "\n",
    "    def on_epoch_end(self, model):\n",
    "        print(f'Epoch {self.epoch}')\n",
    "        self.epoch += 1\n",
    "        \n",
    "class Word2VecTransformer(BaseEstimator, TransformerMixin):\n",
    "    \n",
    "    def __init__(self, w2v_model, alpha=1):\n",
    "        \n",
    "        self.w2v_model = w2v_model\n",
    "        self.alpha = alpha\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X, y=None):\n",
    "        X_transformed = np.zeros((len(X), self.w2v_model.wv.vector_size))\n",
    "        for i, title in enumerate(X):\n",
    "            title_vector = np.zeros((self.w2v_model.wv.vector_size,))\n",
    "            try:\n",
    "                tokens = title.split()\n",
    "            except BaseException:\n",
    "                continue\n",
    "            \n",
    "            counter = 1\n",
    "            \n",
    "            for token in tokens:\n",
    "                if token in self.w2v_model.wv.key_to_index:\n",
    "                    title_vector += self.w2v_model.wv.get_vector(token)\n",
    "                    counter += 1 \n",
    "                    \n",
    "            X_transformed[i] = title_vector / (self.alpha * counter)\n",
    "        \n",
    "        return X_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "753b936e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss after epoch 0: 25454584.0\n",
      "Loss after epoch 10: 735712.0\n",
      "Loss after epoch 20: 840792.0\n",
      "Loss after epoch 30: 893192.0\n",
      "Loss after epoch 40: 902232.0\n",
      "Loss after epoch 50: 892272.0\n",
      "Loss after epoch 60: 894320.0\n",
      "Loss after epoch 70: 859496.0\n",
      "Loss after epoch 80: 845536.0\n",
      "Loss after epoch 90: 0.0\n",
      "Loss after epoch 100: 0.0\n",
      "Loss after epoch 110: 0.0\n",
      "Loss after epoch 120: 0.0\n",
      "Loss after epoch 130: 0.0\n",
      "Loss after epoch 140: 0.0\n",
      "Loss after epoch 150: 0.0\n",
      "Loss after epoch 160: 0.0\n",
      "Loss after epoch 170: 0.0\n",
      "Loss after epoch 180: 0.0\n",
      "Loss after epoch 190: 0.0\n",
      "Loss after epoch 200: 0.0\n",
      "Loss after epoch 210: 0.0\n",
      "Loss after epoch 220: 0.0\n",
      "Loss after epoch 230: 0.0\n",
      "Loss after epoch 240: 0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(289528214, 381662000)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_w2v_corpus = post_df.lemmatized_text.str.split()\n",
    "text_w2v_corpus = text_w2v_corpus\n",
    "text_w2v_model = Word2Vec(sg=1, hs=1, vector_size=10)\n",
    "text_w2v_model.build_vocab(text_w2v_corpus)\n",
    "text_w2v_model.train(\n",
    "    text_w2v_corpus,\n",
    "    total_examples=text_w2v_model.corpus_count,\n",
    "    epochs=250,\n",
    "    compute_loss=True,\n",
    "    callbacks=[LossLogger()]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8a7ce139",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_w2v_model.save(CONFIG['datasets_folder'] + '/text_w2v_model_10')\n",
    "text_word_vectors = text_w2v_model.wv\n",
    "text_word_vectors.save(CONFIG['datasets_folder'] + '/text_w2v_word_vectors_10')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7db9d775",
   "metadata": {},
   "outputs": [],
   "source": [
    "text2vec = Word2VecTransformer(w2v_model=text_w2v_model)\n",
    "w2v_text_transform = text2vec.transform(post_df.lemmatized_text.values)\n",
    "\n",
    "text_w2v_cols = [f'text_w2v_{i}' for i in range(1, 11)]\n",
    "\n",
    "w2v_df = pd.DataFrame(w2v_text_transform, columns=text_w2v_cols)\n",
    "post_df = pd.concat((post_df.reset_index(drop=True), w2v_df.reset_index(drop=True)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2826f5a5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>text</th>\n",
       "      <th>topic</th>\n",
       "      <th>lemmatized_text</th>\n",
       "      <th>text_w2v_1</th>\n",
       "      <th>text_w2v_2</th>\n",
       "      <th>text_w2v_3</th>\n",
       "      <th>text_w2v_4</th>\n",
       "      <th>text_w2v_5</th>\n",
       "      <th>text_w2v_6</th>\n",
       "      <th>text_w2v_7</th>\n",
       "      <th>text_w2v_8</th>\n",
       "      <th>text_w2v_9</th>\n",
       "      <th>text_w2v_10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>UK economy facing major risks  The UK manufact...</td>\n",
       "      <td>business</td>\n",
       "      <td>uk economy facing major risk the uk manufactur...</td>\n",
       "      <td>-0.368771</td>\n",
       "      <td>0.140649</td>\n",
       "      <td>0.088472</td>\n",
       "      <td>0.021023</td>\n",
       "      <td>0.443825</td>\n",
       "      <td>-0.061287</td>\n",
       "      <td>-0.286586</td>\n",
       "      <td>0.539613</td>\n",
       "      <td>0.303215</td>\n",
       "      <td>0.001286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Aids and climate top Davos agenda  Climate cha...</td>\n",
       "      <td>business</td>\n",
       "      <td>aid and climate top davos agenda climate chang...</td>\n",
       "      <td>-0.419259</td>\n",
       "      <td>0.070178</td>\n",
       "      <td>0.138067</td>\n",
       "      <td>-0.007298</td>\n",
       "      <td>0.547702</td>\n",
       "      <td>-0.148632</td>\n",
       "      <td>-0.261452</td>\n",
       "      <td>0.350810</td>\n",
       "      <td>0.175346</td>\n",
       "      <td>-0.051671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Asian quake hits European shares  Shares in Eu...</td>\n",
       "      <td>business</td>\n",
       "      <td>asian quake hit european share share in europe...</td>\n",
       "      <td>-0.376187</td>\n",
       "      <td>0.177674</td>\n",
       "      <td>0.145735</td>\n",
       "      <td>0.042862</td>\n",
       "      <td>0.485289</td>\n",
       "      <td>0.022764</td>\n",
       "      <td>-0.256666</td>\n",
       "      <td>0.491820</td>\n",
       "      <td>0.283088</td>\n",
       "      <td>0.007006</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>India power shares jump on debut  Shares in In...</td>\n",
       "      <td>business</td>\n",
       "      <td>india power share jump on debut share in india...</td>\n",
       "      <td>-0.302412</td>\n",
       "      <td>0.191225</td>\n",
       "      <td>0.093895</td>\n",
       "      <td>0.012854</td>\n",
       "      <td>0.595289</td>\n",
       "      <td>-0.010114</td>\n",
       "      <td>-0.180129</td>\n",
       "      <td>0.466709</td>\n",
       "      <td>0.306224</td>\n",
       "      <td>0.045170</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>Lacroix label bought by US firm  Luxury goods ...</td>\n",
       "      <td>business</td>\n",
       "      <td>lacroix label bought by u firm luxury good gro...</td>\n",
       "      <td>-0.178043</td>\n",
       "      <td>0.146726</td>\n",
       "      <td>0.135657</td>\n",
       "      <td>-0.064135</td>\n",
       "      <td>0.651315</td>\n",
       "      <td>-0.080695</td>\n",
       "      <td>-0.196958</td>\n",
       "      <td>0.340326</td>\n",
       "      <td>0.142488</td>\n",
       "      <td>-0.037439</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   post_id                                               text     topic  \\\n",
       "0        1  UK economy facing major risks  The UK manufact...  business   \n",
       "1        2  Aids and climate top Davos agenda  Climate cha...  business   \n",
       "2        3  Asian quake hits European shares  Shares in Eu...  business   \n",
       "3        4  India power shares jump on debut  Shares in In...  business   \n",
       "4        5  Lacroix label bought by US firm  Luxury goods ...  business   \n",
       "\n",
       "                                     lemmatized_text  text_w2v_1  text_w2v_2  \\\n",
       "0  uk economy facing major risk the uk manufactur...   -0.368771    0.140649   \n",
       "1  aid and climate top davos agenda climate chang...   -0.419259    0.070178   \n",
       "2  asian quake hit european share share in europe...   -0.376187    0.177674   \n",
       "3  india power share jump on debut share in india...   -0.302412    0.191225   \n",
       "4  lacroix label bought by u firm luxury good gro...   -0.178043    0.146726   \n",
       "\n",
       "   text_w2v_3  text_w2v_4  text_w2v_5  text_w2v_6  text_w2v_7  text_w2v_8  \\\n",
       "0    0.088472    0.021023    0.443825   -0.061287   -0.286586    0.539613   \n",
       "1    0.138067   -0.007298    0.547702   -0.148632   -0.261452    0.350810   \n",
       "2    0.145735    0.042862    0.485289    0.022764   -0.256666    0.491820   \n",
       "3    0.093895    0.012854    0.595289   -0.010114   -0.180129    0.466709   \n",
       "4    0.135657   -0.064135    0.651315   -0.080695   -0.196958    0.340326   \n",
       "\n",
       "   text_w2v_9  text_w2v_10  \n",
       "0    0.303215     0.001286  \n",
       "1    0.175346    -0.051671  \n",
       "2    0.283088     0.007006  \n",
       "3    0.306224     0.045170  \n",
       "4    0.142488    -0.037439  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "post_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d16173bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "post_df.to_csv(CONFIG['datasets_folder'] + '/post_data_lemmatized_and_embs.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681adaaf",
   "metadata": {},
   "source": [
    "## Обучим катбуст"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "82e9cc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Соединим исходный датасет действий пользователей с полученными эмбеддингами текстов\n",
    "\n",
    "data = pd.read_csv(CONFIG['datasets_folder'] + '/processed_df.csv')\n",
    "post_df = pd.read_csv(CONFIG['datasets_folder'] + '/post_data_lemmatized_and_embs.csv').drop(['topic'], axis=1)\n",
    "data = data.merge(post_df, on='post_id')\n",
    "\n",
    "text_w2v_cols = [f'text_w2v_{i}' for i in range(1, 11)]\n",
    "\n",
    "columns_order = [\n",
    "    'post_id', 'topic', 'tfidf_sum', \n",
    "    'tfidf_mean', 'tfidf_max', 'user_id',\n",
    "    'gender', 'age', 'country', 'city', \n",
    "    'exp_group', 'os', 'source','month', \n",
    "    'hour', 'day', 'weekday', 'timestamp', 'target'\n",
    "] + text_w2v_cols\n",
    "data = data[columns_order]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "77ad36a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data.drop(['timestamp', 'target', 'user_id', 'post_id'], axis=1)\n",
    "y = data['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0aadeb1d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b496ebb4b5a8481aaa221c52c07761c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MetricVisualizer(layout=Layout(align_self='stretch', height='500px'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "<catboost.core.CatBoostClassifier at 0x2087a67e1d0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Обучим катбуст\n",
    "\n",
    "categorical_features = ['country', 'city', 'topic']\n",
    "catboost_model = CatBoostClassifier(\n",
    "    cat_features=categorical_features,\n",
    "    depth=2,\n",
    "    iterations=250,\n",
    "    verbose=False,\n",
    "    random_seed=42,\n",
    ")\n",
    "\n",
    "catboost_model.fit(X, y, plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e9926401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Качество: 0.6040527398418069\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'0': {'precision': 0.6131435330731188,\n",
       "  'recall': 0.5754138272111725,\n",
       "  'f1-score': 0.5936798320124734,\n",
       "  'support': 2289905},\n",
       " '1': {'precision': 0.5959336612093622,\n",
       "  'recall': 0.6330029113334525,\n",
       "  'f1-score': 0.6139092137629217,\n",
       "  'support': 2265285},\n",
       " 'accuracy': 0.6040527398418069,\n",
       " 'macro avg': {'precision': 0.6045385971412405,\n",
       "  'recall': 0.6042083692723126,\n",
       "  'f1-score': 0.6037945228876975,\n",
       "  'support': 4555190},\n",
       " 'weighted avg': {'precision': 0.6045851053050367,\n",
       "  'recall': 0.6040527398418069,\n",
       "  'f1-score': 0.6037398547642279,\n",
       "  'support': 4555190}}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Грубо оценим качество обученной модели\n",
    "\n",
    "print(f\"Качество: {catboost_model.score(X, y)}\")\n",
    "classification_report(y, catboost_model.predict(X), output_dict=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "274d20d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Качество на трейне: 0.6461308174783567\n"
     ]
    }
   ],
   "source": [
    "print(f\"Качество на трейне: {roc_auc_score(y, catboost_model.predict_proba(X)[:, 1])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fe2e5bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "catboost_model.save_model(CONFIG['data_folder'] + '/catboost_model_with_text_embs', format=\"cbm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba797ad3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
